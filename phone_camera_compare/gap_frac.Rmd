---
title: "Comparing gap fraction estimates from smartphone clip-on fisheye lenses and a DSLR with a fisheye lens"
author: "John Godlee"
date: "05/10/2018"
output: html_document
---

# Preamble

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(error = TRUE)
knitr::opts_chunk$set(fig.align = "center")
```

```{r, message=FALSE, warning=FALSE}
# Packages
library(ggplot2)
library(ggrepel)
library(dplyr)
library(tidyr)
library(jpeg)
library(grid)
library(gridExtra)
```

This first analysis uses a very cheap clip-on lens. The second uses a more sophisticated lens which is integrated into a phone case to stop it moving around on the body of the camera. Unfortunately, both clip on lenses were used in different plots, so direct comparison is difficult. However, the DSLR setup was the same for both analyses.

At this point, I haven't looked into Leaf Area Index measurements, Site Factors, or anything else that relies on weighting gaps depending on their position in the hemisphere. Part of the reason for not doing this is that it's near impossible to know what the projection shape of the clip on lenses is. Some other fisheye clip-ons use an equidistant projection, and I'd wager that these lenses are similar, but I'm not entirely sure. 

Fisheye photos taken pointing straight up to zenith were compared under 10 sample location in oak-ash-hazel plantation.

Photos from the camera and the phone fisheye were assumed to follow an equi-solid projection of the fisheye image onto a flat surface, with a field of view of approximately 180°. These assumptions should be true for the camera lens, but are a guess for the phone clip-on lens.

Photos from this first analysis look something like this:

```{r}
img1 <-  rasterGrob(as.raster(readJPEG("img/hemi_cheap_phone.jpg")), interpolate = FALSE)

img1
```

There is quite a bit of chromatic aberration, and some distortion over the image, not just at the edges.

Photos were sequentially cropped to circles with radii equal to 100%, 90%, 75%, 50% and 25% of the full circular radius of the fisheye image, in order to test the impact of cropping on gap fraction estimates, between phone and camera. 

The plot below details the pixel diameters of cropped circular images on the phone and camera:

```{r}
phone_px_rad <- 2460 / 2
camera_px_rad <- 3925 / 2

crops <- c(1, 0.9, 0.75, 0.5, 0.25)

phone_px_rad <- phone_px_rad * crops
camera_px_rad <- camera_px_rad * crops
rads <- c(phone_px_rad, camera_px_rad)
phone_camera <- c(rep("phone", times = 5), rep("camera", times = 5))
crops_group <- rep(crops, times = 2)

crops_df <- data.frame(crops_group, phone_camera, rads)

ggplot(crops_df, aes(x = crops_group, y = rads, colour = phone_camera)) + 
	geom_point() + 
	geom_line() + 
	scale_x_reverse() + 
	theme_classic() + 
	labs(x = "Proportional crop by diameter of original image", 
		y = "Pixel diameter of resultant image")
```

Cropped images were then converted to 8-bit images and binarised to black and white using the auto-threshold function of ImageJ, with the [Huang](http://www.ktl.elf.stuba.sk/study/vacso/Zadania-Cvicenia/Cvicenie_3/TimA2/Huang_E016529624.pdf) threshold algorithm, which generally provides good differentiation between plant material and sky.

The frequency of black pixels (plant material) was then estimated using ImageJ. This process is automated  with this macro code:

```
// Calculate the gap fraction of a circular selection of an image

// User inputs
///////////////////////////////////

input_path = "/Users/johngodlee/Desktop/input/";

output_path = "/Users/johngodlee/Desktop/output/";

circle_diam = 3925

///////////////////////////////////
// END user inputs


list = getFileList(input_path);

for (i=0; i<(list.length); i++) {

	open(""+input_path+list[i]+"");

	run("8-bit");
	setAutoThreshold("Huang dark");
	run("Convert to Mask");

	makeOval((getWidth/2) - (0.5 * circle_diam),
		(getHeight/2) - (0.5 * circle_diam),
		circle_diam,
		circle_diam);

	file_name = getInfo("image.filename");

	run("Analyze Particles...", "summarize add");

	saveAs("Jpeg", ""+output_path+file_name+"");

	image_id = getImageID();
	selectImage(image_id);
	close();
}

saveAs("Results", ""+output_path+"results.xls");
```

and the total number of pixels in the image (plant and sky) was estimated with this macro code:

```
// Count the total number of pixels in a circular selection, e.g. a full frame hemispherical photo

// User inputs
///////////////////////////////////

input_path = "/Users/johngodlee/Desktop/input/";

output_path = "/Users/johngodlee/Desktop/output/";

circle_diam = 3925

///////////////////////////////////
// END user inputs

list = getFileList(input_path);

for (i=0; i<(list.length); i++) {

	open(""+input_path+list[i]+"");

	run("8-bit");

	setThreshold(0, 255);
	setOption("BlackBackground", false);
	run("Convert to Mask");

	makeOval((getWidth/2) - (0.5 * circle_diam),
		(getHeight/2) - (0.5 * circle_diam),
		circle_diam,
		circle_diam);

	run("Analyze Particles...", "display add");

	image_id = getImageID();
	selectImage(image_id);
	close();
}

saveAs("Results", ""+output_path+"results.xls");
```

The resultant gap fractions used in the rest of this analysis are a ratio black pixels (plant material) to all pixels (plant and sky) in the cropped image.

# Import and fix data

```{r}
cam <- read.csv("gap_frac.csv")

cam$crop_fac <- as.factor(cam$crop)

cam$photo_pair <- paste(cam$photo_no, cam$crop, sep = "_")

cam$photo_no <- as.factor(cam$photo_no)

cam$gap_fraction <- cam$gap_fraction * 100

head(cam, n = 3)
```

Create a summary table of mean gap fraction per crop method:

```{r}
cam_summ <- cam %>%
	group_by(phone_camera, crop) %>%
	summarise(mean_gap_frac = mean(gap_fraction)) %>%
	mutate(crop = as.factor(crop))

head(cam_summ, n = 3)
```

# Plot xy scatter - gap fraction vs. crop level

Looking for flat lines (i.e. gap fraction estimate doesn't change with cropping), especially in the phone group, as cropping is necessary.

Looking for phone and camera estimates to be identical

```{r}
ggplot(cam_summ, aes(x = as.numeric(crop), y = mean_gap_frac)) + 
	geom_point(aes(colour = phone_camera)) + 
	geom_smooth(aes(fill = phone_camera, colour = phone_camera), method = "lm") + 
	labs(x = "Crop (% original diameter)", y = "Mean gap fraction (%)") + 
	theme_classic()
```

```{r}
ggplot(cam, aes(x = phone_camera, y = gap_fraction)) + 
	geom_point(aes(colour = crop_fac, shape = photo_no)) + 
	geom_point(data = cam_summ, 
		aes(x = phone_camera, y = mean_gap_frac), 
		size = 3) + 
	geom_line(data = cam_summ, 
		aes(x = phone_camera, y = mean_gap_frac,
		group = crop),
		size = 1) +	
	geom_label_repel(data = filter(cam_summ, phone_camera == "camera"),
		aes(x = phone_camera, y = mean_gap_frac, label = paste(crop, "° crop", sep = "")), 
		nudge_x = -0.15, nudge_y = c(0, 0.1, 0, 0, 0)) + 
	scale_shape_manual(values = 1:15) + 
	geom_line(aes(x = phone_camera, y = gap_fraction, group = photo_pair, colour = crop_fac), 
		alpha = 0.3) +
	geom_vline(aes(xintercept = 1, alpha = 0.3), 
		linetype = "dashed", show.legend = FALSE) + 
	geom_vline(aes(xintercept = 2, alpha = 0.3), 
		linetype = "dashed", show.legend = FALSE) + 
	labs(x = "", y = "Gap fraction (%)") +
	theme_classic() + 
	theme(axis.text = element_text(size = 10))
```

```{r}
split.spread.t <- function(df) { 
	df_sel = select(df, photo_pair, phone_camera, gap_fraction, crop, crop_fac)
	df_spread = spread(df_sel, phone_camera, gap_fraction)
	
	ldf = split(df_spread, df_spread$crop)
	
	l_test_out = lapply(ldf, function(x) t.test(x$camera, x$phone, paired = TRUE))
	
	l_list = lapply(l_test_out, function(x) data.frame(t = x$statistic, p_value = x$p.value, 
		conf_lo = x$conf.int[1], conf_hi = x$conf.int[2]))
	
	l_df = do.call("rbind", l_list)
	
	l_df$crop = as.numeric(rownames(l_df))
	
	l_df
}

t_out <- split.spread.t(cam)

t_out
```

```{r}
cam_diff <- cam %>%
	select(photo_no, photo_pair, phone_camera, gap_fraction, crop_fac, crop) %>%
	spread(phone_camera, gap_fraction) %>%
	mutate(gap_frac_diff = camera - phone) %>%
	mutate(crop_title = factor(paste("crop: ", crop, "°", sep = ""), levels = c("crop: 25°","crop: 50°","crop: 75°","crop: 90°","crop: 100°")))

ggplot(cam_diff, aes(x = photo_no, y = gap_frac_diff, fill = photo_no)) + 
	geom_bar(stat = "identity", colour = "black") + 
	facet_wrap(~crop_title, scales = "free_x") + 
	theme_classic() + 
	theme(legend.position = "none") + 
	labs(x = "Photo no.", y = "Gap frac. % (camera - phone)")
```

Photo 1 always seems very different, regardless of crop. Here is a comparison of photo 1 from the camera and phone, thresholded and original image:

```{r}
img1 <-  rasterGrob(as.raster(readJPEG("img/hemi_1_camera.jpg")), interpolate = FALSE)
img2 <-  rasterGrob(as.raster(readJPEG("img/hemi_1_phone.jpg")), interpolate = FALSE)

grid.arrange(img1, img2, ncol = 2)
```

```{r}
img1_thresh <-  rasterGrob(as.raster(readJPEG("img/hemi_1_camera_thresh.jpg")), interpolate = FALSE)
img2_thresh <-  rasterGrob(as.raster(readJPEG("img/hemi_1_phone_thresh.jpg")), interpolate = FALSE)

grid.arrange(img1_thresh, img2_thresh, ncol = 2)
```

Basically, the camera image was thresholded poorly due to over-exposure, leading to an under-estimate of gap fraction, and the phone image was a bit blurry leading to an over-estimate of gap fraction.

At 25 degree crop, Photo 8 is very different, again this is because of blurring on the phone, leading to over-estimated gap fraction.

All other pairs, across crops greater than 75 degrees have a reasonably consistent over-estimation of gap fraction by the phone.

I've yet to run this data to calculate leaf area index, but if you're interested, I've been using [HemiPhot](https://github.com/naturalis/Hemiphot), written as an R language port of Winphot, which is now obsolete.

# Second analysis

This analysis has photos looking like this: 

```{r}
img1 <-  rasterGrob(as.raster(readJPEG("img/hemi_exp_phone.jpg")), interpolate = FALSE)

grid.arrange(img1)
```

We didn't look at cropping factors in this analysis, just whether the camera compares to the phone.

First import and fix the data
```{r}
cam_data <- read.csv("cam_data.csv", stringsAsFactors = FALSE)

cam_data$point_no <- letters[cam_data$point_no] 

cam_summ <- cam_data %>%
	group_by(camera_phone) %>%
	summarise(mean_gap_frac = mean(gap_frac))
```

Plot paired bar graphs

```{r}
ggplot(cam_data, aes(x = point_no, y = gap_frac)) + 
	geom_bar(stat = "identity", position = "dodge", aes(fill = camera_phone), width = 0.5) + 
	theme_classic() + 
	labs(x = "Sample location", y = "Gap fraction (%)") + 
	theme(legend.title = element_blank())
```

And an interval plot, similar to the previous analysis, with the black points showing the mean of all photos.

```{r}
ggplot(cam_data, aes(x = camera_phone, y = gap_frac)) + 
	geom_point(aes(colour = point_no), size = 2) + 
	geom_line(aes(x = camera_phone, y = gap_frac, group = point_no), 
		alpha = 0.3) +
	geom_vline(aes(xintercept = 1, alpha = 0.3), 
		linetype = "dashed", show.legend = FALSE) + 
	geom_vline(aes(xintercept = 2, alpha = 0.3), 
		linetype = "dashed", show.legend = FALSE) + 
	geom_point(data = cam_summ, 
		aes(x = camera_phone, y = mean_gap_frac),
		size = 5) + 
	geom_line(data = cam_summ, 
		aes(x = camera_phone, y = mean_gap_frac, group = NA), 
		size = 1) + 
	labs(x = "", y = "Gap fraction (%)") +
	theme_classic() + 
	theme(axis.text = element_text(size = 10), 
		legend.position = "none")
```

While the photos are fairly variable between camera and phone, the means are almost identical, which gives me confidence that any variation between the two methods is a result of random noise, rather than systematic bias.